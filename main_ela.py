# main_ela.py
import os
import json
import base64
from dotenv import load_dotenv
from typing import List

from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, SystemMessage

from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_groq import ChatGroq

from langchain_community.retrievers import BM25Retriever
from langchain_classic.retrievers import EnsembleRetriever

# Configuration
load_dotenv()
DB_PATH = "./chroma_db"
EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"

# V√©rification FlashRank
try:
    from flashrank import Ranker, RerankRequest
    FLASHRANK_AVAILABLE = True
except ImportError:
    FLASHRANK_AVAILABLE = False

class FlashRankCompressor:
    """Custom compressor using FlashRank for reranking documents."""
    def __init__(self, top_n: int = 5):
        self.ranker = Ranker(model_name="ms-marco-MiniLM-L-12-v2", cache_dir="/tmp")
        self.top_n = top_n
    
    def compress_documents(self, documents: List[Document], query: str) -> List[Document]:
        if not documents: 
            return []
        passages = [{"id": i, "text": doc.page_content, "meta": doc.metadata} for i, doc in enumerate(documents)]
        rerank_request = RerankRequest(query=query, passages=passages)
        results = self.ranker.rerank(rerank_request)
        return [documents[result["id"]] for result in results[:self.top_n]]

class ELA_Bot:
    """
    ELA (Econometrics Learning Assistant) - Version Stateless pour Chainlit DataLayer.
    """
    
    def __init__(self):
        print("ü§ñ Initialisation du moteur RAG ELA...")
        
        if not os.path.exists(DB_PATH):
            print(f"‚ùå Erreur : Le dossier '{DB_PATH}' n'existe pas.")
            # En prod, on pourrait lever une erreur, ici on print juste
        
        # 1. Embeddings & VectorDB
        self.embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
        self.vector_db = Chroma(persist_directory=DB_PATH, embedding_function=self.embedding_model)
        
        # 2. Retrievers
        self.retriever = self._build_retrievers()
        
        # 3. LLM
        if "GROQ_API_KEY" not in os.environ:
            print("‚ö†Ô∏è GROQ_API_KEY non d√©finie.")
        
        self.llm = ChatGroq(
            model="meta-llama/llama-4-scout-17b-16e-instruct",
            temperature=0.1,
            max_tokens=2048
        )
        
        # 4. Chain
        self.rag_chain = self._build_chain()
        print("‚úÖ Moteur ELA pr√™t !")

    def _build_retrievers(self):
        chroma_retriever = self.vector_db.as_retriever(search_kwargs={"k": 20})
        
        # BM25 n√©cessite les documents bruts
        all_docs_data = self.vector_db.get()
        docs_list = [
            Document(page_content=txt, metadata=meta)
            for txt, meta in zip(all_docs_data['documents'], all_docs_data['metadatas'])
        ]
        
        # S√©curit√© si la DB est vide
        if not docs_list:
            return chroma_retriever

        bm25_retriever = BM25Retriever.from_documents(docs_list)
        bm25_retriever.k = 20
        
        ensemble_retriever = EnsembleRetriever(
            retrievers=[bm25_retriever, chroma_retriever],
            weights=[0.5, 0.5]
        )
        
        if FLASHRANK_AVAILABLE:
            compressor = FlashRankCompressor(top_n=5)
            class CompressorWrapper:
                def __init__(self, compressor, base_retriever):
                    self.compressor = compressor
                    self.base_retriever = base_retriever
                def invoke(self, query):
                    docs = self.base_retriever.invoke(query)
                    return self.compressor.compress_documents(docs, query)
            return CompressorWrapper(compressor, ensemble_retriever)
        
        return ensemble_retriever

    def _build_chain(self):
        prompt = ChatPromptTemplate.from_messages([
            ("system", """Tu es ELA (Econometrics Learning Assistant), un assistant expert pour √©tudiants de master et strictement limit√© au contenu p√©dagogique fourni.
DIRECTIVE CRITIQUE :
Tu ne poss√®des AUCUNE connaissance en dehors des informations fournies ci-dessous (Contexte et Image).
Tu es amn√©sique concernant l'histoire, la g√©ographie ou la culture g√©n√©rale.

R√àGLES ABSOLUES :
1. Tu dois r√©pondre en utilisant **UNIQUEMENT** les informations pr√©sentes dans le CONTEXTE DU COURS ci-dessous ou dans l'image fournie.
2. **Maths** : Utilise `$...$` (inline) et `$$...$$` (bloc).
3. **Sources** : Cite [Source: Fichier.tex, Slide: Titre].
4. **Incertitude** : Si la r√©ponse √† la question n'est pas explicitement dans le contexte ou l'image, tu dois dire : "Je ne trouve pas cette information dans vos documents de cours."
5. **Conversation** : Utilise UNIQUEMENT l'historique de conversation, les informations pr√©sentes dans le CONTEXTE DU COURS ci-dessous ou dans l'image fournie.
6. **Contexte** : N'utilise JAMAIS tes connaissances externes pour combler un manque d'information (pas d'hallucination).
7. **Style** : P√©dagogique, pr√©cis, rigoureux.
CONTEXTE DU COURS (Source unique) :
{context}"""),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{question}")
        ])

        def format_docs(docs):
            formatted = []
            for doc in docs:
                src = doc.metadata.get('source', 'Inconnu')
                slide = doc.metadata.get('slide_title', 'Sans titre')
                # Nettoyage basique
                content = doc.page_content.replace("\n", " ")
                formatted.append(f">> [Source: {src} | Slide: {slide}]\n{content}")
            return "\n\n".join(formatted)

        # On utilise une m√©thode simple pour combiner le contexte et la question
        chain = (
            {
                "context": lambda x: format_docs(self.retriever.invoke(x["question"])),
                "question": lambda x: x["question"],
                "chat_history": lambda x: x["chat_history"]
            }
            | prompt
            | self.llm
            | StrOutputParser()
        )
        return chain
    
    # NOUVELLE M√âTHODE pour pr√©parer le contexte texte (RAG)
    def _get_rag_context(self, question: str):
        docs = self.retriever.invoke(question)
        formatted = []
        for doc in docs:
            src = doc.metadata.get('source', 'Inconnu')
            content = doc.page_content.replace("\n", " ")
            formatted.append(f">> [Source: {src}]\n{content}")
        return "\n\n".join(formatted)

    # MODIFICATION MAJEURE de la m√©thode ask
    async def ask(self, question: str, chat_history: list = None, image_path: str = None) -> str:
        if chat_history is None: 
            chat_history = []
        
        try:
            # 1. R√©cup√©rer le contexte RAG (Textuel)
            context_text = self._get_rag_context(question)
            
            # 2. Pr√©parer le message Syst√®me
            system_prompt = f"""Tu es ELA, assistant expert en √©conom√©trie.
            R√àGLES :
            1. Analyse l'image fournie si pr√©sente (graphique, √©quation, tableau).
            2. Utilise le CONTEXTE DU COURS ci-dessous pour t'aider.
            3. R√©ponds en fran√ßais avec rigueur math√©matique (LaTeX pour les formules).
            
            CONTEXTE DU COURS :
            {context_text}"""

            messages = [SystemMessage(content=system_prompt)]
            
            # Ajouter l'historique de conversation
            messages.extend(chat_history)

            # 3. Construire le message Utilisateur (Texte + Image potentielle)
            content_blocks = [{"type": "text", "text": question}]
            
            if image_path:
                # Encodage de l'image en Base64 pour l'API Groq
                with open(image_path, "rb") as image_file:
                    image_data = base64.b64encode(image_file.read()).decode('utf-8')
                
                content_blocks.append({
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}
                })
                print("üñºÔ∏è Image d√©tect√©e et envoy√©e √† Groq Vision")

            # Ajouter le message utilisateur final
            messages.append(HumanMessage(content=content_blocks))

            # 4. Appel direct au LLM (On contourne la chaine rigide pour la flexibilit√© Vision)
            response = await self.llm.ainvoke(messages)
            
            return response.content
            
        except Exception as e:
            return f"‚ùå Erreur ELA Vision : {str(e)}"

    # --- NOUVELLE M√âTHODE POUR LE QUIZ ---
    async def generate_quiz_json(self, topic: str, num_questions: int = 3):
        """
        G√©n√®re une liste de questions QCM au format JSON bas√©e sur le cours.
        """
        # 1. R√©cup√©ration du contexte pertinent (RAG)
        context_text = self._get_rag_context(topic)
        
        # 2. Prompt strict pour forcer le JSON
        system_prompt = f"""Tu es un professeur expert en √©conom√©trie.
        Ta t√¢che est de cr√©er un quiz de {num_questions} questions (QCM) sur le sujet : "{topic}".
        
        R√àGLES STRICTES :
        1. Base-toi UNIQUEMENT sur le CONTEXTE DU COURS fourni ci-dessous.
        2. La sortie DOIT √™tre un JSON valide (sans Markdown, sans texte avant/apr√®s).
        3. Format attendu :
        [
            {{
                "question": "Texte de la question ?",
                "options": ["A) R√©ponse 1", "B) R√©ponse 2", "C) R√©ponse 3"],
                "correct_index": 0,
                "explanation": "Courte explication de pourquoi c'est la bonne r√©ponse."
            }},
            ...
        ]

        CONTEXTE DU COURS :
        {context_text}
        """

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=f"G√©n√®re le quiz sur {topic}.")
        ]

        try:
            # Appel LLM avec param√®tre pour forcer le JSON si le mod√®le le supporte (ou via prompt)
            # Pour Groq/Llama, le prompt strict fonctionne g√©n√©ralement bien
            response = await self.llm.ainvoke(messages)
            content = response.content.strip()
            
            # Nettoyage si le LLM ajoute du markdown ```json ... ```
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].strip()

            quiz_data = json.loads(content)
            return quiz_data
            
        except Exception as e:
            print(f"‚ùå Erreur g√©n√©ration Quiz : {e}")
            # Fallback en cas d'erreur de parsing
            return []   
        

    async def generate_practical_code(self, topic: str, language: str = "Python"):
        """
        G√©n√®re un exemple de code pratique bas√© sur la th√©orie du cours.
        """
        # On r√©cup√®re un peu de th√©orie pour guider le mod√®le, mais on compte surtout sur ses capacit√©s de codage
        context_text = self._get_rag_context(topic)
        
        system_prompt = f"""Tu es un Senior Data Scientist et expert en √âconom√©trie.
        Ton but est de fournir un script {language} PARFAITEMENT EX√âCUTABLE et p√©dagogique sur : "{topic}".
        
        R√àGLES DE CODAGE STRICTES (PYTHON) :
        1. **Nommage des variables** : Distingue CLAIREMENT la cible (y) et les features (X). N'appelle jamais ta matrice de design 'X' si 'X' est d√©j√† ta s√©rie temporelle brute. Utilise `y` pour la d√©pendante et `X_design` ou `exog` pour les explicatives.
        2. **Importations** : Importe toutes les librairies n√©cessaires au d√©but.
        3. **Donn√©es** : Le code DOIT g√©n√©rer ses propres donn√©es synth√©tiques (np.random) pour √™tre autonome.
        4. **V√©rification** : Le script ne doit pas contenir d'erreur de syntaxe (comme √©craser une variable utilis√©e ensuite).
        5. **Visualisation** : Inclus un graphique matplotlib clair si pertinent.
        
        CONTEXTE TH√âORIQUE (√† respecter pour la formule) :
        {context_text}
        
        FORMAT DE R√âPONSE :
        - Courte intro.
        - Bloc de code (complet, sans placeholder).
        - Courte interpr√©tation.
        """

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=f"√âcris le script pour {topic} en {language}.")
        ]

        try:
            response = await self.llm.ainvoke(messages)
            return response.content
        except Exception as e:
            return f"‚ùå Erreur de g√©n√©ration de code : {str(e)}"