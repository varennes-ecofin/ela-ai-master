# main_ela.py
import os
import json
import base64
from dotenv import load_dotenv
from typing import List

from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, SystemMessage

from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_groq import ChatGroq

from langchain_community.retrievers import BM25Retriever
from langchain_classic.retrievers import EnsembleRetriever

# Configuration
load_dotenv()
DB_PATH = "./chroma_db"
EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
LLM_MODEL_NAME = "meta-llama/llama-4-scout-17b-16e-instruct"

ELA_BASE_INSTRUCTIONS = """
Tu es ELA (Econometrics Learning Assistant), un assistant expert p√©dagogique.

PROTOCOL DE CITATION STRICT (OBLIGATOIRE) :
Tu dois imp√©rativement indiquer l'origine de chaque information donn√©e. Distingue visuellement les sources :

1. **SOURCE DOCUMENT (Priorit√© Absolue)** : 
    - Si l'information vient du CONTEXTE DU COURS, termine la phrase/paragraphe par : `[Source: Nom_Du_Fichier]`.
    - Ne paraphrase pas sans citer.

2. **SOURCE EXTERNE (Compl√©ment IA)** :
    - Si tu utilises tes propres connaissances (autoris√© UNIQUEMENT selon les r√®gles du MODE EXPERT ci-dessous), termine le paragraphe par : `[Source: Connaissances G√©n√©rales]`.
    - Si une r√©ponse mixe les deux, chaque partie doit avoir son √©tiquette distincte.

DIRECTIVES DE COMPORTEMENT (MODE EXPERT) :

1. **Hi√©rarchie des Connaissances** :
    - **PRIORIT√â 1 (Le Cours)** : En PRIORIT√â ABSOLUE, base-toi sur le CONTEXTE DU COURS, les images et l'historique. Si le cours d√©finit une notation ou une m√©thode sp√©cifique, tu dois la suivre imp√©rativement.
    - **PRIORIT√â 2 (Savoir Sp√©cialis√©)** : Si le contexte est muet, tu es autoris√© √† utiliser tes connaissances (en taguant `[Source: Connaissances G√©n√©rales]`) UNIQUEMENT si le sujet concerne :
        * **S√©ries Temporelles** : ARIMA, VAR, VECM, Stationnarit√©, Coint√©gration, Racine Unitaire, Bruit Blanc...
        * **√âconom√©trie Financi√®re** : Volatilit√© (ARCH/GARCH), Rendements, Gestion des risques financiers.
        * **Code** : Syntaxe Python/R appliqu√©e √† ces sujets sp√©cifiques.

2. **Fronti√®res Strictes (Liste d'Exclusion)** :
    Tu dois REFUSER de traiter les sujets suivants s'ils ne sont pas dans le contexte :
    - **Micro-√©conom√©trie** : Panel, Logit/Probit, Tobit, IV (sauf si contexte S√©ries Temporelles).
    - **Machine Learning G√©n√©raliste** : Classification, Clustering, NLP, Vision.
    - **Culture G√©n√©rale** : Histoire, Politique, etc.

   *R√©action* : Si l'utilisateur pose une question interdite, r√©ponds : "Je suis sp√©cialis√© en S√©ries Temporelles et √âconom√©trie Financi√®re. Ce sujet sort du cadre du cours."

3. **Style & Format** :
    - P√©dagogique, universitaire, rigoureux.
    - Utilise imp√©rativement LaTeX : $...$ (inline) et $$...$$ (bloc).
"""

# V√©rification FlashRank
try:
    from flashrank import Ranker, RerankRequest
    FLASHRANK_AVAILABLE = True
except ImportError:
    FLASHRANK_AVAILABLE = False

class FlashRankCompressor:
    """Custom compressor using FlashRank for reranking documents."""
    def __init__(self, top_n: int = 5):
        self.ranker = Ranker(model_name="ms-marco-MiniLM-L-12-v2", cache_dir="/tmp")
        self.top_n = top_n
    
    def compress_documents(self, documents: List[Document], query: str) -> List[Document]:
        if not documents: 
            return []
        passages = [{"id": i, "text": doc.page_content, "meta": doc.metadata} for i, doc in enumerate(documents)]
        rerank_request = RerankRequest(query=query, passages=passages)
        results = self.ranker.rerank(rerank_request)
        return [documents[result["id"]] for result in results[:self.top_n]]

class ELA_Bot:
    """
    ELA (Econometrics Learning Assistant) - Version Stateless pour Chainlit DataLayer.
    """
    
    def __init__(self):
        print("ü§ñ Initialisation du moteur RAG ELA...")
        
        if not os.path.exists(DB_PATH):
            print(f"‚ùå Erreur : Le dossier '{DB_PATH}' n'existe pas.")
            # En prod, on pourrait lever une erreur, ici on print juste
        
        # 1. Embeddings & VectorDB
        self.embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
        self.vector_db = Chroma(persist_directory=DB_PATH, embedding_function=self.embedding_model)
        
        # 2. Retrievers
        self.retriever = self._build_retrievers()
        
        # 3. LLM
        if "GROQ_API_KEY" not in os.environ:
            print("‚ö†Ô∏è GROQ_API_KEY non d√©finie.")
        
        self.llm = ChatGroq(
            model=LLM_MODEL_NAME,
            temperature=0.2,
            max_tokens=2048
        )
        
        # 4. Chain
        self.rag_chain = self._build_chain()
        print("‚úÖ Moteur ELA pr√™t !")

    def _build_retrievers(self):
        chroma_retriever = self.vector_db.as_retriever(search_kwargs={"k": 20})
        
        # BM25 n√©cessite les documents bruts
        all_docs_data = self.vector_db.get()
        docs_list = [
            Document(page_content=txt, metadata=meta)
            for txt, meta in zip(all_docs_data['documents'], all_docs_data['metadatas'])
        ]
        
        # S√©curit√© si la DB est vide
        if not docs_list:
            return chroma_retriever
        
        # Fonction de pr√©traitement pour ignorer la casse
        def case_insensitive_tokenizer(text):
            return text.lower().split()

        bm25_retriever = BM25Retriever.from_documents(docs_list,preprocess_func=case_insensitive_tokenizer)
        bm25_retriever.k = 20
        
        ensemble_retriever = EnsembleRetriever(
            retrievers=[bm25_retriever, chroma_retriever],
            weights=[0.5, 0.5]
        )
        
        if FLASHRANK_AVAILABLE:
            compressor = FlashRankCompressor(top_n=5)
            class CompressorWrapper:
                def __init__(self, compressor, base_retriever):
                    self.compressor = compressor
                    self.base_retriever = base_retriever
                def invoke(self, query):
                    docs = self.base_retriever.invoke(query)
                    return self.compressor.compress_documents(docs, query)
            return CompressorWrapper(compressor, ensemble_retriever)
        
        return ensemble_retriever

    def _build_chain(self):
        # On combine la base + le format attendu par LangChain
        system_prompt_text = ELA_BASE_INSTRUCTIONS + """
        
        CONTEXTE DU COURS (Source unique) :
        {context}
        """
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt_text),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{question}")
        ])

        def format_docs(docs):
            formatted = []
            for doc in docs:
                src = doc.metadata.get('source', 'Inconnu')
                slide = doc.metadata.get('slide_title', 'Sans titre')
                # Nettoyage basique
                content = doc.page_content.replace("\n", " ")
                formatted.append(f">> [Source: {src} | Slide: {slide}]\n{content}")
            return "\n\n".join(formatted)

        # On utilise une m√©thode simple pour combiner le contexte et la question
        chain = (
            {
                "context": lambda x: format_docs(self.retriever.invoke(x["question"])),
                "question": lambda x: x["question"],
                "chat_history": lambda x: x["chat_history"]
            }
            | prompt
            | self.llm
            | StrOutputParser()
        )
        return chain
    
    # NOUVELLE M√âTHODE pour pr√©parer le contexte texte (RAG)
    def _get_rag_context(self, question: str):
        docs = self.retriever.invoke(question)
        formatted = []
        for doc in docs:
            src = doc.metadata.get('source', 'Inconnu')
            content = doc.page_content.replace("\n", " ")
            formatted.append(f">> [Source: {src}]\n{content}")
        return "\n\n".join(formatted)

    # MODIFICATION MAJEURE de la m√©thode ask
    async def ask(self, question: str, chat_history: list = None, image_path: str = None) -> str:
        if chat_history is None: 
            chat_history = []
        
        try:
            # 1. R√©cup√©rer le contexte RAG (Textuel)
            context_text = self._get_rag_context(question)
            
            # 2. Pr√©parer le message Syst√®me (VERSION RENFORC√âE)
            full_system_prompt = f"""{ELA_BASE_INSTRUCTIONS}

            T√ÇCHE ACTUELLE :
            R√©ponds √† la question de l'√©tudiant.
            
            RAPPEL CRITIQUE SUR LES SOURCES :
            - Chaque affirmation doit √™tre sourc√©e.
            - Utilise `[Source: ...]` pour le RAG.
            - Utilise `[Source: Connaissances G√©n√©rales]` si l'info vient de ton propre savoir.
            - Si l'info est dans le contexte RAG ci-dessous, la citation est OBLIGATOIRE.
            
            CONTEXTE DU COURS (RAG) :
            {context_text}"""

            messages = [SystemMessage(content=full_system_prompt)]
            messages.extend(chat_history)

            # 3. Construire le message Utilisateur (Texte + Image potentielle)
            content_blocks = [{"type": "text", "text": question}]
            
            if image_path:
                # Encodage de l'image en Base64 pour l'API Groq
                with open(image_path, "rb") as image_file:
                    image_data = base64.b64encode(image_file.read()).decode('utf-8')
                
                content_blocks.append({
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}
                })
                print("üñºÔ∏è Image d√©tect√©e et envoy√©e √† Groq Vision")

            # Ajouter le message utilisateur final
            messages.append(HumanMessage(content=content_blocks))

            # 4. Appel direct au LLM (On contourne la chaine rigide pour la flexibilit√© Vision)
            response = await self.llm.ainvoke(messages)
            
            return response.content
            
        except Exception as e:
            return f"‚ùå Erreur ELA Vision : {str(e)}"


    # --- METHODE POUR LE QUIZ ---
    async def generate_quiz_json(self, topic: str, num_questions: int = 3):
        """
        G√©n√®re une liste de questions QCM au format JSON sans LaTeX pour √©viter les bugs.
        """
        context_text = self._get_rag_context(topic)
        
        # --- CHANGEMENT MAJEUR : PROMPT D√âDI√â SANS LATEX ---
        # On n'utilise PAS ELA_BASE_INSTRUCTIONS ici pour ne pas h√©riter de l'obligation LaTeX.
        quiz_system_prompt = f"""
        Tu es ELA, un assistant expert p√©dagogique.
        
        T√ÇCHE : Cr√©er un quiz QCM de {num_questions} questions sur le sujet : "{topic}".

        R√àGLES DE CONTENU (RAG) :
        1. Base-toi UNIQUEMENT sur le CONTEXTE DU COURS ci-dessous.
        2. Si le sujet n'est pas dans le cours, renvoie un JSON vide.

        R√àGLES DE FORMAT (CRITIQUE POUR √âVITER LES BUGS) :
        1. **INTERDICTION TOTALE DU LATEX**. N'utilise jamais de symboles avec des backslashs.
        2. √âcris les concepts math√©matiques en TOUTES LETTRES ou en notation standard.
        3. La sortie doit √™tre STRICTEMENT un objet JSON valide (RFC 8259).
        
        FORMAT ATTENDU :
        [
            {{
                "question": "Quelle est la d√©finition du R-carr√© ?",
                "options": ["A) La variance...", "B) La moyenne...", "C) ..."],
                "correct_index": 0,
                "explanation": "Le R-carr√© repr√©sente..."
            }}
        ]

        CONTEXTE DU COURS :
        {context_text}
        """

        messages = [
            SystemMessage(content=quiz_system_prompt),
            HumanMessage(content=f"G√©n√®re le quiz sur {topic} sans LaTeX.")
        ]

        try:
            # Appel LLM avec param√®tre pour forcer le JSON si le mod√®le le supporte (ou via prompt)
            # Pour Groq/Llama, le prompt strict fonctionne g√©n√©ralement bien
            response = await self.llm.ainvoke(messages)
            content = response.content.strip()
            
            # Nettoyage si le LLM ajoute du markdown ```json ... ```
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].strip()

            quiz_data = json.loads(content)
            return quiz_data
            
        except Exception as e:
            print(f"‚ùå Erreur g√©n√©ration Quiz : {e}")
            # Fallback en cas d'erreur de parsing
            return []   
        

    async def generate_practical_code(self, topic: str, language: str = "Python"):
        """
        G√©n√®re un exemple de code pratique bas√© sur la th√©orie du cours.
        """
        # On r√©cup√®re un peu de th√©orie pour guider le mod√®le, mais on compte surtout sur ses capacit√©s de codage
        context_text = self._get_rag_context(topic)
        
        # ICI UNE NUANCE : On garde la base d'instruction mais on ajoute la comp√©tence CODAGE
        full_system_prompt = f"""{ELA_BASE_INSTRUCTIONS}

        EXCEPTION : Pour cette t√¢che de programmation, tu as le droit d'utiliser tes connaissances en syntaxe {language} (librairies, fonctions), MAIS les √©quations et la logique th√©orique doivent venir strictement du CONTEXTE DU COURS.
        
        T√ÇCHE ACTUELLE : G√âN√âRER UN SCRIPT {language} EX√âCUTABLE
        Sujet : "{topic}"
        
        R√àGLES DE CODAGE STRICTES (PYTHON) :
        1. **Nommage des variables** : Distingue CLAIREMENT la cible (y) et les features (X). N'appelle jamais ta matrice de design 'X' si 'X' est d√©j√† ta s√©rie temporelle brute. Utilise `y` pour la d√©pendante et `X_design` ou `exog` pour les explicatives.
        2. **Importations** : Importe toutes les librairies n√©cessaires au d√©but.
        3. **Donn√©es** : Le code DOIT g√©n√©rer ses propres donn√©es synth√©tiques (np.random) pour √™tre autonome.
        4. **V√©rification** : Le script ne doit pas contenir d'erreur de syntaxe (comme √©craser une variable utilis√©e ensuite).
        5. **Visualisation** : Inclus un graphique matplotlib clair si pertinent.
        
        CONTEXTE TH√âORIQUE (√† respecter pour la formule) :
        {context_text}
        
        FORMAT DE R√âPONSE :
        - Courte intro.
        - Bloc de code (complet, sans placeholder).
        - Courte interpr√©tation.
        """

        messages = [
            SystemMessage(content=full_system_prompt),
            HumanMessage(content=f"√âcris le script pour {topic} en {language}.")
        ]

        try:
            response = await self.llm.ainvoke(messages)
            return response.content
        except Exception as e:
            return f"‚ùå Erreur de g√©n√©ration de code : {str(e)}"